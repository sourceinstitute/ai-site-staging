Next, as part of this series of lessons about industry readiness, we're
looking at the issue of bias (in this case, race-based bias) in AI and how
that impacts its use.

> "As artificial intelligence and machine learning mature and manifest their
potential to take on complicated tasks, we've become somewhat expectant that
robots can succeed where humans have failed -- namely, in putting aside
personal biases when making decisions. But as recent cases have shown, like
all disruptive technologies, machine learning introduces its own set of
unexpected challenges and sometimes yields results that are wrong, unsavory,
offensive and not aligned with the moral and ethical standards of human
society."

>

> \- [Ben Dickson writing in Techcrunch](https://techcrunch.com/2016/11/07
/why-its-so-hard-to-create-unbiased-artificial-intelligence/)

Or, put more simply…

> "We think artificial intelligence is impartial. Often, it's not."

>

> -Brian Resnick at [Vox.com](http://vox.com/)

Does that mean AI itself is prejudice?

> "A lot of people are saying this is showing that AI is prejudiced. No. This
is showing we're prejudiced and that AI is learning it."

>

> -Joanna Bryson, computer scientist at the University of Bath, quoted in [The
Guardian](https://www.theguardian.com/technology/2017/apr/13/ai-programs-
exhibit-racist-and-sexist-biases-research-reveals)

So AI is vulnerable to our human prejudices. That same Guardian article gave
this example:

> "Research, published in the journal
[Science](http://science.sciencemag.org/content/356/6334/183), focuses on a
machine learning tool known as "word embedding", which is already transforming
the way computers interpret speech and text… the AI system was more likely to
associate European American names with pleasant words such as "gift" or
"happy", while African American names were more commonly associated with
unpleasant words."

We encountered this problem when facilitating a workshop between an
entrepreneur and a data scientist on how to predict payday loan defaults with
a random forest algorithm. One of the data points we had about each borrower
was their race. Thus the question was raised, should we include this in the
data our algorithm would analyze?

Our conclusion was not to. But why? What if the race of the applicant turned
out to be the best predictor of loan default? Wouldn't we be sabotaging our
algorithm by denying it this information? The fixer Valyo Yolovski explained
it this way:

> "They might be underrepresented in society, so they are poor and they have
to take out more payday loans than the rest of the people and there you have
it. It's not because they're Muslim [it was pointed out that Muslim applicants
had the highest default rates], it's because of their social standing. But
they cannot get out because you're actually profiling them even more now. So
you're actually going to be perpetuating that. So race is not a good
predictor. It's a standard example of bias, noise in the data confuses
everything."

In other words, labels like race or religion often simply mask the actual
relevant data beneath them. Valyo explained in the fixer session that these
kinds of easy correlations can lead an algorithm away from the deeper
phenomena you're trying to understand. Still, it's not always that easy,
Sandra Wachter, a researcher in data ethics and algorithms at the University
of Oxford pointed out in that same Guardian article that:

> "The question of how to eliminate inappropriate bias from algorithms
designed to understand language, without stripping away their powers of
interpretation, would be challenging."

But that process starts with being aware of the fact that AI will reflect our
biases and actively working against that tendency whenever we use it.



If you'd like to unsubscribe, [click here](http://<unsubscribe>Unsubscribe
here</unsubscribe>). If you have feedback on how we can improve, please reply!
:)

